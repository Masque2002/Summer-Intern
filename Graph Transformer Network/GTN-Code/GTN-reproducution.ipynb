{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preparing work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "包括超参数设置和数据处理。\n",
    "\n",
    "本次复现采用的数据集是ACM数据集，包含8994个点和25922条边，有三种节点类型和四种边的类型，节点特征维度是1902维。一共有3025个有标签的节点，其中划分训练集600，验证集300，测试集2125。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Azusa\\AppData\\Local\\Temp\\ipykernel_20296\\447564189.py:5: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n",
      "  edges = pickle.load(f)\n",
      "C:\\Users\\Azusa\\AppData\\Local\\Temp\\ipykernel_20296\\447564189.py:5: DeprecationWarning: Please use `csc_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csc` namespace is deprecated.\n",
      "  edges = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "with open('data/ACM/node_features.pkl', 'rb') as f:\n",
    "    node_features = pickle.load(f)  # (8994, 1902)\n",
    "with open('data/ACM/edges.pkl', 'rb') as f:  # 多个异构图的邻接矩阵\n",
    "    edges = pickle.load(f)\n",
    "with open('data/ACM/labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "num_nodes = edges[0].shape[0]\n",
    "# 每一个邻接矩阵边的长度是节点总数 有四种边 所以有四个边的邻接矩阵\n",
    "# labels 由3个矩阵组成 分别代表训练集验证集测试集 labels一共只有3025个 因为半监督学习就是 一部分数据有标签 大部分没有 通过有的去划分训练集验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# edges组合成一个张量\n",
    "for i, edge in enumerate(edges):\n",
    "    if i == 0:\n",
    "        A = torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)\n",
    "    else:\n",
    "        tmp = torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)\n",
    "        A = torch.cat([A, tmp], dim=-1)\n",
    "A = torch.cat([A, torch.eye(num_nodes).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)\n",
    "\n",
    "node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "train_node = torch.from_numpy(np.array(labels[0][:, 0])).type(torch.LongTensor)\n",
    "train_target = torch.from_numpy(np.array(labels[0])[:, 1]).type(torch.LongTensor)\n",
    "valid_node = torch.from_numpy(np.array(labels[1])[:, 0]).type(torch.LongTensor)\n",
    "valid_target = torch.from_numpy(np.array(labels[1])[:, 1]).type(torch.LongTensor)\n",
    "test_node = torch.from_numpy(np.array(labels[2])[:, 0]).type(torch.LongTensor)\n",
    "test_target = torch.from_numpy(np.array(labels[2])[:, 1]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape:  torch.Size([8994, 8994, 5])\n",
      "node_features shape: torch.Size([8994, 1902])\n",
      "train_node shape: torch.Size([600])\n",
      "valid_node.shape: torch.Size([300])\n",
      "test_node.shape torch.Size([2125])\n"
     ]
    }
   ],
   "source": [
    "print('A shape: ', A.shape)\n",
    "print('node_features shape:', node_features.shape)\n",
    "print('train_node shape:', train_node.shape)\n",
    "print('valid_node.shape:', valid_node.shape)\n",
    "print('test_node.shape', test_node.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "epochs = 40\n",
    "node_dim = 64\n",
    "num_channels = 2\n",
    "lr = 0.005\n",
    "weight_decay = 0.001\n",
    "num_layers = 2\n",
    "norm = True\n",
    "num_classes = torch.max(train_target).item()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_edge, num_channels, w_in, w_out, num_class, num_layers, norm):\n",
    "        super(GTN, self).__init__()\n",
    "        self.num_edge = num_edge\n",
    "        self.num_channels = num_channels\n",
    "        self.w_in = w_in\n",
    "        self.w_out = w_out\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = num_layers\n",
    "        self.is_norm = norm\n",
    "        layers = []\n",
    "        for i in range(num_layers):  # layers是多个GTlayer组成的 表示要聚合几次meta-path\n",
    "            if i == 0:\n",
    "                layers.append(GTLayer(num_edge, num_channels, first=True))  # 第一层gt layer\n",
    "            else:\n",
    "                layers.append(GTLayer(num_edge, num_channels, first=False))  # 第二层gt layer\n",
    "        self.layers = nn.ModuleList(layers)  # layers定义完成\n",
    "        self.weight = nn.Parameter(torch.Tensor(w_in, w_out))  # GCN的参数\n",
    "        self.bias = nn.Parameter(torch.Tensor(w_out))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.linear1 = nn.Linear(self.w_out * self.num_channels, self.w_out)  # 多个channel拼接在一起 (2*64, 64)\n",
    "        self.linear2 = nn.Linear(self.w_out, self.num_class)  # 最终输出\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)  # gloria初始化\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def gcn_conv(self, X, H):\n",
    "        X = torch.mm(X, self.weight)\n",
    "        H = self.norm(H, add=True)  # 这里的add设置为true 是因为gcn中邻接矩阵要加上I\n",
    "        return torch.mm(H.t(), X)\n",
    "\n",
    "    def normalization(self, H):\n",
    "        for i in range(self.num_channels):   # H的维度是 (2, 8994, 8994) 要对每一个channel的H做归一化\n",
    "            if i == 0:\n",
    "                H_ = self.norm(H[i, :, :]).unsqueeze(0)   # 对H[0]做归一化\n",
    "            else:\n",
    "                H_ = torch.cat((H_, self.norm(H[i, :, :]).unsqueeze(0)), dim=0)  # 对H[1]做归一化然后再拼接到一起\n",
    "        return H_\n",
    "\n",
    "    def norm(self, H, add=False):\n",
    "        H = H.t()\n",
    "        if add == False:\n",
    "            H = H * ((torch.eye(H.shape[0]) == 0).type(torch.FloatTensor))  # 在这里去掉了对角线上的值 即自连接边 因为自连接边的产生是在Q1Q2相乘得到A^(1)的时候产生的 因此是在这里去掉 也就是得到A^(1)之后 与Q3相乘得到A^(2)之前\n",
    "        else:\n",
    "            H = H * ((torch.eye(H.shape[0]) == 0).type(torch.FloatTensor)) + torch.eye(H.shape[0]).type(\n",
    "                torch.FloatTensor)  # 在进入到gcn的运算的时候 add为true 因为gcn中对邻接矩阵要加上一个单位矩阵I\n",
    "        deg = torch.sum(H, dim=1)  # shape: (8994,)\n",
    "        deg_inv = deg.pow(-1)  # 得到 D-1 但此时还是一维的 只是数值是D-1\n",
    "        deg_inv[deg_inv == float('inf')] = 0  # 对角线原来是0取倒数后变为inf 这里重新置为0\n",
    "        deg_inv = deg_inv * torch.eye(H.shape[0]).type(torch.FloatTensor)  # 重新转成二维的 即真正的D-1\n",
    "        H = torch.mm(deg_inv, H)\n",
    "        H = H.t()\n",
    "        return H\n",
    "\n",
    "    def forward(self, A, X, target_x, target):\n",
    "        A = A.unsqueeze(0).permute(0, 3, 1, 2)  # 加一维之后再变换维度 (1, 8994, 8994, 5) -> (1, 5, 8994, 8994) 个人看法认为是因为 后续的卷积层的计算里面 没有一个concat操作\n",
    "                                                # 也就是说如果这里是三维送进去 出来的是三维的 可以看conv层的forward函数 没有concat操作 是直接和卷积核相乘的 所以在这里预先把A变成四维的\n",
    "                                                # 不然没法计算 这个1后续通过计算会变成2 也就是output_channel\n",
    "        Ws = []\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                H, W = self.layers[i](A)\n",
    "            else:\n",
    "                H = self.normalization(H)  # 对A^(1)要先归一化 D-1 * A\n",
    "                H, W = self.layers[i](A, H)\n",
    "            Ws.append(W)  # Ws是卷积层的参数\n",
    "\n",
    "        for i in range(self.num_channels):  # 每个channel做一遍gcn\n",
    "            if i == 0:\n",
    "                X_ = F.relu(self.gcn_conv(X, H[i]))  # X是节点特征矩阵\n",
    "            else:\n",
    "                X_tmp = F.relu(self.gcn_conv(X, H[i]))\n",
    "                X_ = torch.cat((X_, X_tmp), dim=1)\n",
    "        X_ = self.linear1(X_)\n",
    "        X_ = F.relu(X_)\n",
    "        y = self.linear2(X_[target_x])\n",
    "        loss = self.loss(y, target)\n",
    "        return loss, y, Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, first=True):\n",
    "        super(GTLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.first = first\n",
    "        if self.first == True:  # 为什么这里有一个判断是否是第一层的变量 因为第一层要分别两次卷积得到两个Q矩阵 而后续只需要得到一个跟上面的结果拼起来就可以了\n",
    "            self.conv1 = GTConv(in_channels, out_channels)  # W1\n",
    "            self.conv2 = GTConv(in_channels, out_channels)  # W2\n",
    "        else:\n",
    "            self.conv1 = GTConv(in_channels, out_channels)  # W3\n",
    "\n",
    "    def forward(self, A, H_=None):\n",
    "        if self.first == True:\n",
    "            a = self.conv1(A)   # a.shape (2, 8994, 8994)\n",
    "            b = self.conv2(A)   # b.shape (2, 8994, 8994)\n",
    "            H = torch.bmm(a, b)  # 第一次矩阵相乘得到A^(1) 批相乘算法 在这里就是每个channel对应做矩阵乘\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach(), (F.softmax(self.conv2.weight, dim=1)).detach()]  # s\n",
    "        else:\n",
    "            a = self.conv1(A)\n",
    "            H = torch.bmm(H_, a)\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach()]\n",
    "        return H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GTConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, 1, 1))  # 1*1的卷积核 起到降维的作用\n",
    "        self.bias = None\n",
    "        self.scale = nn.Parameter(torch.Tensor([0.1]), requires_grad=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        n = self.in_channels\n",
    "        nn.init.constant_(self.weight, 0.1)  # 初始化参数为常量\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, A):\n",
    "        '''\n",
    "        0. 对weight(conv)做softmax\n",
    "        1. 对每个节点在每个edgeType上进行[2, 5, 1, 1]的卷积操作\n",
    "        2. 对每个edgeType进行加权求和\n",
    "        # F.softmax(self.weight, dim=1)对self.weight做softmax:[2, 5, 1, 1]\n",
    "        # A: [1, 5, 8994, 8994] * [2, 5, 1, 1] -> [2, 5, 8994, 8994]\n",
    "        # sum: [2, 8994, 8994]\n",
    "        '''\n",
    "        A = torch.sum(A * F.softmax(self.weight, dim=1), dim=1)  # 对k=5这一维做了softmax操作\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    return (pred == target).sum().item() / target.numel()\n",
    "\n",
    "def true_positive(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)    \n",
    "\n",
    "def true_negative(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "def false_positive(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "def false_negative(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)\n",
    "\n",
    "def precision(pred, target, num_classes):\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def recall(pred, target, num_classes):\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "def f1_score(pred, target, num_classes):\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f1 = 0\n",
    "model = GTN(num_edge=A.shape[-1],  # 边的种类 4+1\n",
    "            num_channels=num_channels,\n",
    "            w_in=node_features.shape[1],  # 节点特征维数 1902\n",
    "            w_out=node_dim,  # 隐层输出维度 64\n",
    "            num_class=num_classes,\n",
    "            num_layers=num_layers,  # 层数\n",
    "            norm=norm)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train & Valid & Test\n",
    "best_val_loss = 10000\n",
    "best_test_loss = 10000\n",
    "best_train_loss = 10000\n",
    "best_train_f1 = 0\n",
    "best_val_f1 = 0\n",
    "best_test_f1 = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] > 0.005:\n",
    "            param_group['lr'] = param_group['lr'] * 0.9\n",
    "    print('Epoch:  ', i + 1)\n",
    "    model.zero_grad()\n",
    "    model.train()  # A (8994, 8994, 5)\n",
    "    loss, y_train, Ws = model(A, node_features, train_node, train_target)\n",
    "    train_f1 = torch.mean(\n",
    "        f1_score(torch.argmax(y_train.detach(), dim=1), train_target, num_classes=num_classes)).cpu().numpy()\n",
    "    print('Train - Loss: {}, Macro_F1: {}'.format(loss.detach().cpu().numpy(), train_f1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    # Valid\n",
    "    with torch.no_grad():\n",
    "        val_loss, y_valid, _ = model.forward(A, node_features, valid_node, valid_target)\n",
    "        val_f1 = torch.mean(\n",
    "            f1_score(torch.argmax(y_valid, dim=1), valid_target, num_classes=num_classes)).cpu().numpy()\n",
    "        print('Valid - Loss: {}, Macro_F1: {}'.format(val_loss.detach().cpu().numpy(), val_f1))\n",
    "        test_loss, y_test, W = model.forward(A, node_features, test_node, test_target)\n",
    "        test_f1 = torch.mean(\n",
    "            f1_score(torch.argmax(y_test, dim=1), test_target, num_classes=num_classes)).cpu().numpy()\n",
    "        print('Test - Loss: {}, Macro_F1: {}\\n'.format(test_loss.detach().cpu().numpy(), test_f1))\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_loss = val_loss.detach().cpu().numpy()\n",
    "        best_test_loss = test_loss.detach().cpu().numpy()\n",
    "        best_train_loss = loss.detach().cpu().numpy()\n",
    "        best_train_f1 = train_f1\n",
    "        best_val_f1 = val_f1\n",
    "        best_test_f1 = test_f1\n",
    "print('---------------Best Results--------------------')\n",
    "print('Train - Loss: {}, Macro_F1: {}'.format(best_train_loss, best_train_f1))\n",
    "print('Valid - Loss: {}, Macro_F1: {}'.format(best_val_loss, best_val_f1))\n",
    "print('Test - Loss: {}, Macro_F1: {}'.format(best_test_loss, best_test_f1))\n",
    "final_f1 += best_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}